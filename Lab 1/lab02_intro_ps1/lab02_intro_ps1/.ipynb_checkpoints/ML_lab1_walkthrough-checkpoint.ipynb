{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 | Regressions | Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this walkthrough, our goal will be to use a regression to predict the salary of an NHL player. For this, we will use a dataset containing various metrics about every player in the pre-cited league during the 2016-17 season.\n",
    "\n",
    "Credit: [data source](http://www.hockeyabstract.com/testimonials/nhl2016-17playerdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 | Exploratory Data Analysis and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is inherently needed when performing machine learning, so the first step in any project is to load and expore the data. The exploration, commonly referred to as Exploratory Data Analysis (EDA) is essential, as it helps us spot weaknesses in the data (duplicate rows, missing data, outliers, etc.) and can guide our decision in how to manipulate it before fitting a model (or while trying to improve it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>born</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>position</th>\n",
       "      <th>games_played</th>\n",
       "      <th>goals</th>\n",
       "      <th>assists</th>\n",
       "      <th>plus_minus</th>\n",
       "      <th>penalty_minutes</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spencer Abbott</td>\n",
       "      <td>1988</td>\n",
       "      <td>69</td>\n",
       "      <td>170</td>\n",
       "      <td>LW</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>$575'000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Justin Abdelkader</td>\n",
       "      <td>1987</td>\n",
       "      <td>74</td>\n",
       "      <td>218</td>\n",
       "      <td>LW</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>-20</td>\n",
       "      <td>50</td>\n",
       "      <td>$5'500'000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pontus Aberg</td>\n",
       "      <td>1993</td>\n",
       "      <td>71</td>\n",
       "      <td>196</td>\n",
       "      <td>LW</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>$842'500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Noel Acciari</td>\n",
       "      <td>1991</td>\n",
       "      <td>70</td>\n",
       "      <td>208</td>\n",
       "      <td>C</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>$892'500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kenny Agostino</td>\n",
       "      <td>1992</td>\n",
       "      <td>72</td>\n",
       "      <td>202</td>\n",
       "      <td>LW</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>$625'000.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  born  height  weight position  games_played  goals  \\\n",
       "0     Spencer Abbott  1988      69     170       LW             1      0   \n",
       "1  Justin Abdelkader  1987      74     218       LW            64      7   \n",
       "2       Pontus Aberg  1993      71     196       LW            15      1   \n",
       "3       Noel Acciari  1991      70     208        C            29      2   \n",
       "4     Kenny Agostino  1992      72     202       LW             7      1   \n",
       "\n",
       "   assists  plus_minus  penalty_minutes         salary  \n",
       "0        0           0                0    $575'000.00  \n",
       "1       14         -20               50  $5'500'000.00  \n",
       "2        1          -2                4    $842'500.00  \n",
       "3        3           3               16    $892'500.00  \n",
       "4        2           0                2    $625'000.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(888, 11)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "nhl = pd.read_csv('nhl_2016_17.csv')\n",
    "display(nhl.head())\n",
    "print(nhl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with a dataset, often the first challenge is understanding what each of the rows mean. Thus, it is often advised to carefully read the data-dictionary. In our case, the columns represent the following:\n",
    "- `name`: A player's name\n",
    "- `born`: A player's year of birth\n",
    "- `heigh`: A player's height in inches\n",
    "- `weight`: A player's weight in pounds\n",
    "- `position`: A player's position\n",
    "- `games_played`: The number of games a player played during the season\n",
    "- `goals`: The number of goals scored by the player during the season\n",
    "- `assists`: The number of assists recorded by the player during the season\n",
    "- `plus_minus`: The number of times a player was on the ice when his team scored a goal minus the number of times he was on the ice when the opposing team scored a goal at even strength or short-handed\n",
    "- `penalty_minutes`: The number of penalty minutes the player collected\n",
    "- `salary`: A player's salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After understanding what each column measures, one must understand which type of data is in each column as our treatment/interpretation of them depends on it. As you know, there are four types of data:\n",
    "\n",
    "- Numerical:\n",
    "    - Discrete: integer numbers (e.g., 5, 6, 7) → distinctness, order, addition, multiplication\n",
    "    - Continuous: float numbers (e.g., 5.25, 5.50, 5.75) → distinctness, order, addition, multiplication\n",
    "- Categorical:\n",
    "    - Ordinal: distinct and can be ordered (e.g., low, medium, high) → distinctness, order\n",
    "    - Nominal: distinct but cannot be ordered (e.g., male, female) → distinctness\n",
    "\n",
    "The distinction between those categories is not always obvious and each category bring different attributes with them:\n",
    "\n",
    "- Distinctness: =, ≠\n",
    "- Order: <, >\n",
    "- Addition: +, -\n",
    "- Multiplication: ×, ÷\n",
    "\n",
    "Note that since numerical discrete and numerical continuous have the same characteristics, their distinction is often vague. Therefore, most of the time, if a variable can take many different integer values, it is called numerical continuous.\n",
    "\n",
    "In our case, the types of data are the following:\n",
    "\n",
    "| Types of Data | Columns |\n",
    "| :---: | :---: |\n",
    "| Numerical discrete | `born` |\n",
    "| Numerical continuous | `height`, `weight`, `games_played`, `goals`, `assists`, `plus_minus`, `penalty_minutes`, `salary` |\n",
    "| Categorical Ordinal | None here :-( |\n",
    "| Categorical Nominal | `position`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we truly understand our variables, we can start with the EDA. One thing to look at are the missing variables, which one needs to decide how to handle on a case-by-case basis. This can inolve:\n",
    "\n",
    "- Deleting an entire column: If too many values are missing and the column is not critical\n",
    "- Deleting rows with one, some, or all variables missing: If only a small persentage of rows are affected on important variables\n",
    "- Manually collect the missing data: Tideous and could introduce measurement deviations\n",
    "- Input the mean, median, or an arbitrary number: Sometimes hard to justify and required expert knowledge\n",
    "\n",
    "We notice that there are 14 players with missing values for their salary. Since 14/888 rows are not a lot, we will simply drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                0\n",
      "born                0\n",
      "height              0\n",
      "weight              0\n",
      "position            0\n",
      "games_played        0\n",
      "goals               0\n",
      "assists             0\n",
      "plus_minus          0\n",
      "penalty_minutes     0\n",
      "salary             14\n",
      "dtype: int64\n",
      "remaining number of rows: 874\n"
     ]
    }
   ],
   "source": [
    "# Lookup the missing values\n",
    "print(nhl.isna().sum())\n",
    "\n",
    "# Remove the players with missing salaries\n",
    "nhl.dropna(axis=0, subset='salary', inplace=True)\n",
    "print(f'remaining number of rows: {len(nhl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to look at are the datatypes of the columns and check that they are in accordance with what we expect.\n",
    "\n",
    "We notice that the numerical variables are in `int64` while the categorical variables are in `object`, which is as expected. The one expection is the `salary` column, which is a string when it should be a number. Hence, it is something to fix before we can fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name               object\n",
      "born                int64\n",
      "height              int64\n",
      "weight              int64\n",
      "position           object\n",
      "games_played        int64\n",
      "goals               int64\n",
      "assists             int64\n",
      "plus_minus          int64\n",
      "penalty_minutes     int64\n",
      "salary             object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>born</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>position</th>\n",
       "      <th>games_played</th>\n",
       "      <th>goals</th>\n",
       "      <th>assists</th>\n",
       "      <th>plus_minus</th>\n",
       "      <th>penalty_minutes</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spencer Abbott</td>\n",
       "      <td>1988</td>\n",
       "      <td>69</td>\n",
       "      <td>170</td>\n",
       "      <td>LW</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>575000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Justin Abdelkader</td>\n",
       "      <td>1987</td>\n",
       "      <td>74</td>\n",
       "      <td>218</td>\n",
       "      <td>LW</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>-20</td>\n",
       "      <td>50</td>\n",
       "      <td>5500000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pontus Aberg</td>\n",
       "      <td>1993</td>\n",
       "      <td>71</td>\n",
       "      <td>196</td>\n",
       "      <td>LW</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>4</td>\n",
       "      <td>842500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Noel Acciari</td>\n",
       "      <td>1991</td>\n",
       "      <td>70</td>\n",
       "      <td>208</td>\n",
       "      <td>C</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>892500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kenny Agostino</td>\n",
       "      <td>1992</td>\n",
       "      <td>72</td>\n",
       "      <td>202</td>\n",
       "      <td>LW</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>625000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  born  height  weight position  games_played  goals  \\\n",
       "0     Spencer Abbott  1988      69     170       LW             1      0   \n",
       "1  Justin Abdelkader  1987      74     218       LW            64      7   \n",
       "2       Pontus Aberg  1993      71     196       LW            15      1   \n",
       "3       Noel Acciari  1991      70     208        C            29      2   \n",
       "4     Kenny Agostino  1992      72     202       LW             7      1   \n",
       "\n",
       "   assists  plus_minus  penalty_minutes     salary  \n",
       "0        0           0                0   575000.0  \n",
       "1       14         -20               50  5500000.0  \n",
       "2        1          -2                4   842500.0  \n",
       "3        3           3               16   892500.0  \n",
       "4        2           0                2   625000.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the datatypes of all columns\n",
    "print(nhl.dtypes)\n",
    "\n",
    "# Transform the salary column to numeric\n",
    "nhl['salary'] = pd.to_numeric(\n",
    "    nhl['salary'].apply(\n",
    "        lambda sal: str(sal).strip('$').replace(\"'\", '')\n",
    "    )\n",
    ")\n",
    "nhl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then turn our attention to the numerical variables. With `df.describe()`, we can easily get a lot of summary statistics for them. This is a way to get a good sense for the data and find potential outliers. However, plotting might be just as good to do this.\n",
    "\n",
    "Note that here we use histograms, which are ususally the standard (especially when it comes to looking at the general distribution and spotting outliers), but scatterplots, boxplots, etc. are also powerful visualizations.\n",
    "\n",
    "We will log-transform the `penalty_minutes` variable to show you how it is done, even if in this case it makes little sense. However, it is a good example as it forces you to think about how to treat values that cannot be log-transformmed (i.e., <= 0). One way to deal with them is to delete the observations (which we use here) while one could also replace the 0 with an arbitrarily small positive number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display is used to show the output when a command follows it\n",
    "display(nhl.describe())\n",
    "\n",
    "# Plot all numerical variables\n",
    "for col in nhl.select_dtypes(['int64', 'float64']).columns:\n",
    "    nhl.plot(y=col, kind='hist', title=col)\n",
    "\n",
    "# Log-transform penalty_minutes\n",
    "nhl = nhl.loc[nhl['penalty_minutes'] > 0]\n",
    "nhl['penalty_minutes'] = np.log(nhl['penalty_minutes'])\n",
    "nhl.rename(columns={'penalty_minutes': 'ln_penalty_minutes'}, inplace=True)\n",
    "\n",
    "display(nhl.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can turn our attentions to the categorical variables and examine those. Luckily, the examination does not change much from the numerical variables.\n",
    "\n",
    "Obviously, something needs to be done to categorical variables for them to be interpretable by a model. A common approach is to use dummy variables (also called one-hot encoding, label encoding being the other option) where a column is replaced by a binary column for each possible values. In our case, there are four possible values for the `position` so we will have just as many dummy columns.\n",
    "\n",
    "However, this creates multicolinearity as, exactly one of `position_C`, `position_D`, `position_LW`, or `position_RW` will take the value `1` and the others will take the value `0`. Hence, we will have to remove one of the variables from the regression later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the categorical column\n",
    "nhl['position'].value_counts().plot(y=col, kind='bar', title=col)\n",
    "\n",
    "# Create the dummy variable for the position\n",
    "nhl = nhl.join(pd.get_dummies(nhl['position'], prefix='position'))\n",
    "\n",
    "nhl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All along the data cleaning process, you should always keep track of the number of rows and columns. This might seem trivial, but it is very easy that a method you use duplicates the rows, creates unnecessary index columns, etc. This is easily done with `len(df)`\n",
    "\n",
    "Another thing you should always check is that you don't have any duplicate rows (if they should not exist in your data). If you have an `id` column (`name` in our case), you can simply use `df['id'].value_counts().max()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows\n",
    "print(f'Number of rows: {len(nhl)}')\n",
    "\n",
    "# Check for duplicates on the name\n",
    "print(f'Most occurances of the same name: {nhl[\"name\"].value_counts().max()}')\n",
    "\n",
    "# Display the data\n",
    "display(nhl.head())\n",
    "print(nhl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyways, we have finally finished the data cleaning! Remember, this is usually 80% of the work... so now, let's turn to the actual machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 | Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, in your statistics classes, you have always run regressions on the entire dataset. This can easily be done in python with the `statsmodels` library.\n",
    "\n",
    "Let's imagine that we want to predict `salary` with `goals` and a constant: $ \\boxed{salary_i = \\beta_0 + \\beta_1 \\cdot goals_i + \\varepsilon_i} $\n",
    "\n",
    "As one can see, the output gives us all of the metrics we are used to and if a player scores no goals, his expected salary is $1'393'000 and each additional goal adds an additional $129'700 to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress the salary on goals and a constant\n",
    "regression = smf.ols('salary ~ goals', data=nhl).fit()\n",
    "print(regression.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily visualize this regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.regplot(x='goals', y='salary', data=nhl)\n",
    "plt.xlabel(\"Goals\", fontsize=15)\n",
    "plt.ylabel(\"Salary\", fontsize=15)\n",
    "plt.title('Salary vs Goals', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one can of course remove the constant to get to the following regression: $ \\boxed{salary_i = \\beta \\cdot goals_i + \\varepsilon_i} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the constant\n",
    "regression = smf.ols('salary ~ goals - 1', data=nhl).fit()\n",
    "print(regression.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more variables is also quite straightforward. For instance, if we were to run the following regression: $ \\boxed{salary_i = \\beta_0 + \\beta_1 \\cdot goals_i + \\beta_2 \\cdot games\\_played_i + \\varepsilon_i} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress the salary on goals, games_played, and a constant\n",
    "regression = smf.ols('salary ~ goals + games_played', data=nhl).fit()\n",
    "print(regression.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get a few easy cases out of the way:\n",
    "- Using log-transformed variables is quite easy, because nothing expect the interpretation of the coefficients changes.\n",
    "- We can also use interaction effects by multiplying the variables (e.g., goals × assists).\n",
    "- Finally, one can also use higher-order coefficients (e.g., games_played^2).\n",
    "\n",
    "For instance, if we were to run the regression below, how would you interpret the coefficients?\n",
    "\n",
    "$ \\boxed{salary_i = \\beta_0 + \\beta_1 \\cdot ln(penalty\\_minutes) + \\beta_2 \\cdot goals_i + \\beta_3 \\cdot assists_i + \\beta_4 \\cdot goals_i \\cdot assists_i + \\beta_5 \\cdot games\\_played^2 + \\varepsilon_i} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = smf.ols('salary ~ ln_penalty_minutes + goals*assists + np.power(games_played, 2)', data=nhl).fit()\n",
    "print(regression.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our categorical variables, remember that we created dummy variables. However, we have to watch out for multicolinarity, which is why we will leave out the position_C variable.\n",
    "\n",
    "In this case, the coefficients of the position dummy variables represent the change in expected salary of a player compared to the case where he is a center (position_C = 1). For instance, if a player is a defenseman (position_D = 1), he will on average earn $854'700 more than if he were a center.\n",
    "\n",
    "$ \\boxed{salary_i = \\beta_0 + \\beta_1 \\cdot goals_i + \\beta_2 \\cdot games\\_played_i + \\beta_3 \\cdot ln\\_penalty\\_minutes + \\beta_4 \\cdot position\\_C + \\beta_5 \\cdot position\\_LW + \\beta_6 \\cdot position\\_RW + \\varepsilon_i} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = smf.ols('salary ~ goals + games_played + ln_penalty_minutes + position_D + position_LW + position_RW', data=nhl).fit()\n",
    "print(regression.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that we do not necessarily have to create the dummy variables ourselves as the models often do it for us with categorical variables. However, if we wanted to create a dummy variable for a certain threshold (e.g., players born before/after 1990), we would have to manually create the dummy variables just as we did.\n",
    "\n",
    "Note two things:\n",
    "- The regression does drop a (random) possible value of `position` to avoid multicolinearity\n",
    "- The coefficients are the same as when we use the dummy variables\n",
    "\n",
    "Hence, manually creating dummy variables on categorical variables has the advantage of letting us choose which potential variable to drop to avoid multicolinearity, which is often important for interpretability (you want to drop the baseline value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression without the manual dummy variables\n",
    "regression = smf.ols('salary ~ goals + games_played + ln_penalty_minutes + position', data=nhl).fit()\n",
    "print(regression.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we have horribly disregarded one of the fundamental principles of machine learning; the train-test split. Actually, with (almost) all machine learning models we will use throught the course, you will follow the following steps:\n",
    "\n",
    "1. Load the data and clean it\n",
    "2. Split the data into training and test sets\n",
    "3. Data-normalization\n",
    "4. Create/fit the regression\n",
    "5. Metrics to evaluate the model (R2, MAE, MSE)\n",
    "6. Retrieve the coefficients\n",
    "7. Make predictions with the model\n",
    "\n",
    "Luckily, we have already covered step 1 in length, so let's get right into step 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Splitting the dataset\n",
    "\n",
    "Sklearn has a very useful module to seprate your dataset in a training and in a testing set. The training set will be used to retreive the best values of the weights according to a combination of input/output while the test set will be used to evaluate/predict our model. Since our model will be trained on particular values we want to test our data on a new set of data (the test set)\n",
    "\n",
    "The test size here is of 20% of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the target variable\n",
    "features = 'goals', 'games_played', 'ln_penalty_minutes', 'position_D', 'position_LW', 'position_RW'\n",
    "y = nhl[['salary']]\n",
    "X = nhl[['goals', 'games_played', 'ln_penalty_minutes', 'position_D', 'position_LW', 'position_RW']]\n",
    "\n",
    "# Split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,      # 20% of the data in the test set\n",
    "    random_state=12,    # Makes sure the split is the same every time\n",
    "    shuffle=True        # Shuffles the rows before splitting\n",
    ")\n",
    "\n",
    "display(X_train)\n",
    "display(y_train)\n",
    "\n",
    "display(X_test)\n",
    "display(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Normalize the Data\n",
    "\n",
    "Generally you should normalize the data right after splitting the datset. The normalization is important here to reduce the variance of our model and get better results. We skip this step for now.\n",
    "\n",
    "The sklearn has several tools to normalize the data. For instance, one can use the MinMaxScaler module to normalize the data. This estimator scales and translates each feature individually such that it is in the given range on the training set (e.g. between zero and one).\n",
    "\n",
    "This is an example of how to use it:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the train and the test set\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# The two last steps above can be merged into one\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "With this code, the X_train data is now bounded between 0 and 1, however, that is not necessarily true for the X_test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Create and Fit the Model\n",
    "\n",
    "To predict the target variable we will use a simple linear regression as we have done previously. For this, we will first create the model (stipulate the parameters) before fitting it to the training data.\n",
    "\n",
    "You will find that we do not have the same coefficients as before. This is because we are not running the regression on the same set of data (remember, we did the train-test split). However, if you were to fit the model on X and y, you would find the exact same coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = LinearRegression(fit_intercept= True)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the coefficients\n",
    "print(f'Intercept: {model.intercept_[0]}')\n",
    "print(f'Feature coefficients: \\n{pd.concat([pd.DataFrame(X.columns), pd.DataFrame(np.transpose(model.coef_))], axis=1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 | Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model, it is time to evaluate it. For this, we commonly use three metrics: MAE, MSE, and R2.\n",
    "\n",
    "We can use these metrics on both the training and the test data. However, it is mostly the test metrics we are interested in. This is logical as the goal of machine learning is to make predictions on data never seen before.\n",
    "\n",
    "One would expect the training metrics to be better than the testing metrics because that is what the model has been fitted upon. However, that is not necessarily the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "predictions = model.predict(X_test)\n",
    "print(f'Test MAE:\\t{mean_absolute_error(y_test, predictions)}')\n",
    "print(f'Test MSE:\\t{mean_squared_error(y_test, predictions)}')\n",
    "print(f'Test R2:\\t{r2_score(y_test, predictions)}\\n')\n",
    "\n",
    "# Evaluate on the training set\n",
    "predictions = model.predict(X_train)\n",
    "print(f'Train MAE:\\t{mean_absolute_error(y_train, predictions)}')\n",
    "print(f'Train MSE:\\t{mean_squared_error(y_train, predictions)}')\n",
    "print(f'Train R2:\\t{r2_score(y_train, predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, one can use these metrics to select the best model (i.e., the one with the best training metrics). To illustrate how this works on an oversimplified example, we will see how the training and testing MAE evolve as we add features to the model (starting with just the year of birth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays to save the different errors\n",
    "train_err = []\n",
    "test_err = []\n",
    "\n",
    "# Features to add iteratively\n",
    "features = [\n",
    "    'born', 'height', 'weight', 'games_played', 'goals',\n",
    "    'assists', 'plus_minus', 'ln_penalty_minutes', 'salary',\n",
    "    'position_D', 'position_LW', 'position_RW'\n",
    "]\n",
    "\n",
    "# Iterate over the number of features\n",
    "for nbr_col in range(1, len(features)):\n",
    "    # Select the good number of features for X\n",
    "    X_temp = X[X.columns[:nbr_col]]\n",
    "\n",
    "    # Split the dat set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_temp,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=7  # Change the random state and see the effect on the errors\n",
    "    )\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create the linear model\n",
    "    LR = LinearRegression(fit_intercept=False)\n",
    "\n",
    "    # Fit the linear model\n",
    "    LR.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute the MAEs\n",
    "    train_err.append(mean_absolute_error(y_train, LR.predict(X_train)))\n",
    "    test_err.append(mean_absolute_error(y_test, LR.predict(X_test)))\n",
    "\n",
    "# Visualize\n",
    "plt.title('Training and Test Errors as Features are Added')\n",
    "plt.plot(range(1, len(features)), train_err, label='Train MAE')\n",
    "plt.plot(range(1, len(features)), test_err, label='Test MAE')\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model does perform better after more than 4 features, so here we would take the first four features for our final model.\n",
    "\n",
    "However, in a real scenario, we would use different approaches to determine which features those should be (not necessarily start with the year of birth, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 | Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after having found the optimal model, we can start doing what machine learning is for; predictions.\n",
    "\n",
    "We have actually already done so when evaluating the models with `model.predict(X)` and it really is that simple.\n",
    "\n",
    "Let's say we would like to predict what a right winger would expect to make if he scored 12 goals in 55 games and had 6 penalty minutes. The model in this case predicts a salary of $2'421'108."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our simple model from the beginning\n",
    "features = ['goals', 'games_played', 'ln_penalty_minutes', 'position_D', 'position_LW', 'position_RW']\n",
    "y = nhl[['salary']]\n",
    "X = nhl[features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=12, shuffle=True\n",
    ")\n",
    "\n",
    "model = LinearRegression(fit_intercept= True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict our imaginary player\n",
    "model.predict(np.array([12, 55, np.log(6), 0, 0, 1]).reshape(1, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b317e399d43dbd8b33fb49d9b48f6247f952c5619db5c5e6e1ad1b6332ff9dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
