{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Resampling & Model Selection\n",
    "\n",
    "The goal of this lab is to give you the tools to:\n",
    "\n",
    " - Perform bootstrapping\n",
    " - Use Leave One Out Cross-Validation (LOOCV)\n",
    " - Perform feature selection with:\n",
    "    - Best Subset Selection\n",
    "    - Forward Stepwise Selection\n",
    "    - Backward Stepwise Selection\n",
    " - Undestand Shrinkage\n",
    "    - Least Absolute Shrinkage and Selection Operator (LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our well-trusted libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sci-kit learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Iterator building blocks\n",
    "# combinations('ABCD', 2) --> AB AC AD BC BD CD\n",
    "from itertools import combinations\n",
    "\n",
    "# Many were concerned with warnings. The short answer is that FutureWarning (most common)\n",
    "# appears when a functionality is deprecated and will be replaced. Here's how to ignore them\n",
    "# even if you should find a way to resolve them in a production environment.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this lab, we will use an extremely simple dataset as this helps with the intuition. In essence, we will try to determine whether a person will default (i.e., fail to make their interest or principal payments on a debt) with their account balance, income, and a student indicator. More precisely:\n",
    "\n",
    " - `default` (str, binary): Whether the individual defaulted\n",
    " - `student` (str, binary): Whether the individual is a student\n",
    " - `balance` (float, continuous): The individual's account balance\n",
    " - `income` (float, continuous): The individual's annual income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance        income\n",
       "1      No      No   729.526495  44361.625074\n",
       "2      No     Yes   817.180407  12106.134700\n",
       "3      No      No  1073.549164  31767.138947\n",
       "4      No      No   529.250605  35704.493935\n",
       "5      No      No   785.655883  38463.495879"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "default\n",
       "No     9667\n",
       "Yes     333\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default = pd.read_csv('default.csv',index_col=0)\n",
    "\n",
    "display(default.head())\n",
    "print(default.shape, '\\n')\n",
    "default.value_counts('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn the `default` and `student` into binary variables. We're only using best practices after all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default  student      balance        income\n",
       "1        0        0   729.526495  44361.625074\n",
       "2        0        1   817.180407  12106.134700\n",
       "3        0        0  1073.549164  31767.138947\n",
       "4        0        0   529.250605  35704.493935\n",
       "5        0        0   785.655883  38463.495879"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_dict = {'Yes': 1,'No': 0}\n",
    "for col in ['default', 'student']:\n",
    "    default[col] = default[col].map(encoding_dict)\n",
    "\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go into resampling methods, let's estimate the model with the original sample (i.e, `default`) to get a sense of the estimates of the coefficients as well as the standard errors associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   const      balance        income\n",
       "1    1.0   729.526495  44361.625074\n",
       "2    1.0   817.180407  12106.134700\n",
       "3    1.0  1073.549164  31767.138947\n",
       "4    1.0   529.250605  35704.493935\n",
       "5    1.0   785.655883  38463.495879"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "Name: default, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Wed, 07 Feb 2024   Pseudo R-squ.:                  0.4594\n",
      "Time:                        11:01:51   Log-Likelihood:                -789.48\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                4.541e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
      "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
      "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "X = default[['balance', 'income']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = default['default']\n",
    "\n",
    "display(X.head(), y.head())\n",
    "\n",
    "results = sm.Logit(y, X).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are interested in the estimate and the standard error of each coefficient. We will now see how resampling methods perform while keeping this as our initial finding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Bootstrapping is a method for estimating sampling distributions of an estimator by resampling with replacement form the original sample. It is a flexible way to quantify uncertainty associated with an estimator or a model.\n",
    "\n",
    "We use bootstrapping to estimate hard to calculate standard errors (or confidence intervals). It is also used when the population is too small to generate a large sample.\n",
    "\n",
    "Regarding the latter, bootstrapping is especially useful because it allows us to mimic the process of selecting many observations from the population.\n",
    "\n",
    "Throughout, we will use the following terminology when referring to the samples:\n",
    "\n",
    "- **Original Sample:** The one sample of size `n` we have from the population\n",
    "- **Subsample:** We draw `B` subsamples (of size `n`) with replacement from the original sample\n",
    "\n",
    "The bootstrap algorithm goes as follows:\n",
    "\n",
    "1. Obtain the original sample from the population\n",
    "2. Draw a subsample from the original sample\n",
    "3. Compute the estimate ($ \\hat{\\alpha}_{b}^{*} $) from the subsample\n",
    "4. Repeat steps 2 and 3 `B` times\n",
    "5. Compute $ SE_{B}(\\hat{\\alpha}) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{\\alpha}_{b}^{*} - \\bar{\\alpha}_{b}^{*})^{2}} $ where $ \\bar{\\alpha}_{b}^{*} = \\frac{\\sum_{b=1}^{B} \\hat{\\alpha}_{b}^{*}}{B}$\n",
    "\n",
    "![Bootstrap Algorithm](bootstrap_algorithm.png)\n",
    "\n",
    "The intuition behind the bootstrap is that we approximate the real distribution in the population by the original sample. This of course relies on the distribution in the original sample to approximate the population (which increases with n).\n",
    "\n",
    "It is hard to overstate the importance of the bootstrap. As Efron and Hastie write, “Bootstrap confidence intervals are neither exact nor optimal, but aim instead for a wide applicability, combined with near-exact accuracy”.\n",
    "\n",
    "There is of course the debate about how many bootstrap replications, B, we should use. Ideally, an infinite amount but that is not feasible because of computational restraints. Therefore, we will settle for “as many as possible”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyways, let's get into the code to replicate the bootstrap algorithm we just looked at. Step 1 is already done as `default` is our original sample of length `n = 10000`. Hence, we move on to step 2 which is to draw a subsample with `n` observations from the original sample.\n",
    "\n",
    "To do this, we will get randomly get the indices of `n` observations with replacement from the original sample. This means that an index can appear 0, 1, or multiple times in our subsample.\n",
    "\n",
    "Note that we could also directly draw a subsample (i.e., make a dataframe) but we decided to do it with indices here and build the dataframe only when we need it in the `boot_fn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_indices in module __main__:\n",
      "\n",
      "get_indices(data, n)\n",
      "    Generates a random subsample (i.e., its indices)\n",
      "    with replacement consisting of n observations each.\n",
      "    \n",
      "    Inputs:\n",
      "        - data (pd.Dataframe): data to sample from\n",
      "        - n (int): numeber of observations in the sample\n",
      "    \n",
      "    Output:\n",
      "        - indices (np.ndarray): array of indices forming\n",
      "            the samples\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2807, 4453, 3137, 9623, 2444])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_indices(data, n):\n",
    "    '''\n",
    "    Generates a random subsample (i.e., its indices)\n",
    "    with replacement consisting of n observations each.\n",
    "\n",
    "    Inputs:\n",
    "        - data (pd.Dataframe): data to sample from\n",
    "        - n (int): numeber of observations in the sample\n",
    "    \n",
    "    Output:\n",
    "        - indices (np.ndarray): array of indices forming\n",
    "            the samples\n",
    "    '''\n",
    "    assert type(data) == pd.DataFrame\n",
    "    assert type(n) == int, 'n must be an integer'\n",
    "\n",
    "    indices = np.random.choice(\n",
    "        data.index,         # Indices as the input\n",
    "        int(n),             # Number of indices per sample\n",
    "        replace=True        # Draw samples with replacement\n",
    "    )\n",
    "    return indices\n",
    "\n",
    "help(get_indices)\n",
    "get_indices(default, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, there is replacement, meaning that an observation can occur 0, 1, or multiple times in the subsample. Here's proof that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9985,     3],\n",
       "       [ 9986,     1],\n",
       "       [ 9987,     1],\n",
       "       [ 9988,     3],\n",
       "       [ 9989,     4],\n",
       "       [ 9992,     1],\n",
       "       [ 9996,     1],\n",
       "       [ 9997,     2],\n",
       "       [ 9998,     1],\n",
       "       [10000,     1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(np.unique(get_indices(default, 10000), return_counts=True)).T[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are already onto step 3, which is to estimate the coefficients with the subsample. Essentially, we do the exact same thing as previously when we used `logit` to get a sense of the ballpark in which the estimates and their standard error lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of a single subsample:\n",
      "\tIntercept:\t-10.84\n",
      "\tBalance:\t0.01\n",
      "\tIncome:\t\t0.0\n"
     ]
    }
   ],
   "source": [
    "# similar to boot.fn in the exercise\n",
    "def boot_fn(data, index, constant=True, features=['balance','income'], target='default'):\n",
    "    '''\n",
    "    Runs a logistic regression (with a constant) on only the specified\n",
    "    indices within the data (i.e., on a single subsample).\n",
    "\n",
    "    Inputs:\n",
    "        - data (pd.Dataframe): data to sample from\n",
    "        - indices (np.ndarray): array of indices forming the samples\n",
    "        - features (lst of str): the name of the features\n",
    "        - target (str): the name of the target\n",
    "\n",
    "    Output:\n",
    "        - coefficients (lst of float): the coefficients\n",
    "    '''\n",
    "    X = data[features].loc[index]\n",
    "    if constant:\n",
    "        X = sm.add_constant(X)\n",
    "    y = data[target].loc[index]\n",
    "    \n",
    "    lr = sm.Logit(y,X).fit(disp=0)\n",
    "    coefficients = [lr.params[0], lr.params[1], lr.params[2]]\n",
    "\n",
    "    return coefficients\n",
    "\n",
    "intercept, coef_balance, coef_income = boot_fn(default, get_indices(default, 10000))\n",
    "print(f'Coefficients of a single subsample:\\n\\tIntercept:\\t{round(intercept, 2)}\\n\\tBalance:\\t{round(coef_balance, 2)}\\n\\tIncome:\\t\\t{round(coef_income, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, `boot_fn` returns the coefficients when running a logistic regression on the indices we give it. As seen, we will use the `get_incidces` function to generate the subsamples (of size `n = len(default)` from `default` with replacement) we will use in the `boot_fn`. It is all coming together!\n",
    "\n",
    "Essentially, we know have the building blocks to retrieve the coefficients of a single subsample. Hence, we can now turn our attention to steps 4 and 5.\n",
    "\n",
    "In step 4, we want to create subsamples and estimate their coefficients before getting the (mean and) standard deviation of those coefficients in step 5. We will do both in the `boot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot(data, func, B):\n",
    "    '''\n",
    "    Executing a bootstrap over B subsamples\n",
    "    to estimate the (mean of) coefficients\n",
    "    and their associated standard errors.\n",
    "\n",
    "    Inputs:\n",
    "        - data (pd.Dataframe): data to sample from\n",
    "        - func (fn): function executing the regression\n",
    "        - B (int): number of subsamples\n",
    "    \n",
    "    Ouput:\n",
    "        - restults (dict of dicts): bootstrapped coefficients\n",
    "            and the associated standard errors\n",
    "    '''\n",
    "    # Step 4\n",
    "    coef_intercept = []\n",
    "    coef_balance = []\n",
    "    coef_income = []\n",
    "\n",
    "    coefs = ['intercept', 'balance', 'income']\n",
    "    output = {coef: [] for coef in coefs}\n",
    "    for i in range(B):\n",
    "        reg_out = func(data, get_indices(data, len(data)))\n",
    "        for i, coef in enumerate(coefs):\n",
    "            output[coef].append(reg_out[i])\n",
    "\n",
    "    # Step 5\n",
    "    results = {}\n",
    "    for coef in coefs:\n",
    "        results[coef] = {\n",
    "            'estimate': np.mean(output[coef]),\n",
    "            'std_err': np.std(output[coef])\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally run our own bootstrap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:\n",
      "\tEstimate: -11.574272256706507\n",
      "\tStandard Error: 0.44334486119778566\n",
      "Balance:\n",
      "\tEstimate: 0.0056709914595677955\n",
      "\tStandard Error: 0.00023220993749622107\n",
      "Income:\n",
      "\tEstimate: 2.06279795372501e-05\n",
      "\tStandard Error: 4.858236373191578e-06\n"
     ]
    }
   ],
   "source": [
    "results = boot(default, boot_fn, 1000)\n",
    "for i, x in results.items():\n",
    "    print(f\"{i.capitalize()}:\\n\\tEstimate: {x['estimate']}\\n\\tStandard Error: {x['std_err']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get values that are very similar to what we expected (i.e., from the logistic regression on the original sample). However, by using it, we reduce the risk of overfitting and improve the stability of our machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave One Out Cross-Validation\n",
    "\n",
    "We already went over cross-validation in previous labs. However, we have yet to cover its most extreme version; Leave One Out Cross Validation (LOOCV).\n",
    "\n",
    "As its name suggests, LOOCV is simply cross validation where the test set contains a single observation. There are a few pros and cons as to why you would choose to use cross-validation:\n",
    "\n",
    "- **Pros:**\n",
    "    - Low Upward Bias: because you only lose one observation when fitting the model on the training data\n",
    "    - Not Noisy: The validation estimate of the test error is always the same\n",
    "- **Cons:**\n",
    "    - Computationally Costly: Requires estimating the model `n` different times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iternation Milestones:...\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "\n",
      "[0.0005927380823778704, 0.00044313658761178823, 0.004082543662994585, 3.6509339069982594e-05, 0.0008426104370810332, 0.0010518241573572432, 0.0007861782470870673, 0.000504295884156107, 0.006093171030518981, 0.006208461720247789]\n",
      "\n",
      "MSE using LOOCV: 0.02824\n"
     ]
    }
   ],
   "source": [
    "squared_errors = []\n",
    "\n",
    "# Iterate over the entire dataset one observation at a time\n",
    "print('Iternation Milestones:...')\n",
    "for i in default.index:\n",
    "    # All except observation i is our training set\n",
    "    train = default.iloc[default.index != i,:]\n",
    "    # Only observation i is our test set\n",
    "    test = default.iloc[default.index == i,:]\n",
    "\n",
    "    # Fit the model and gather the squared error\n",
    "    ols = LinearRegression().fit(train[['balance', 'income']], train['default'])\n",
    "    test_predicted = ols.predict(test[['balance', 'income']])\n",
    "    test_actual = test[['default']]\n",
    "    squared_error = np.power(test_predicted - test_actual, 2)\n",
    "\n",
    "    # Store the squared error\n",
    "    squared_errors.append(squared_error.values[0][0])\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(f'\\n{squared_errors[:10]}\\n')\n",
    "\n",
    "# From the squared errors, get the Mean Squared Error (MSE)\n",
    "print(f'MSE using LOOCV: {round(np.mean(squared_errors), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation stays the same as previously. However, notice how certain errors are way larger than others? Those are outliers, but they average out over the size of dataset so we don't need to give them much attention.\n",
    "\n",
    "Let's make a quick step back to k-fold cross validation. An alternative way of doing it is with the `cross_val_score` that is also in the sci-kit learn library. Here is how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02939107 -0.02840277 -0.02662848 -0.02813761 -0.02856849]\n",
      "0.028225683737308254\n"
     ]
    }
   ],
   "source": [
    "### CC UPDATE:  In this example, there is NO shuffling before the folds are determined, \n",
    "### so 5FCV returns the same thing regardless of seed\n",
    "\n",
    "# In k-fold CV, the seed matters. But not so much if our dataset is big enough\n",
    "np.random.seed(5)\n",
    "\n",
    "X = default[['balance','income']]\n",
    "Y = default['default']\n",
    "ols = LinearRegression()\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    ols,    # Specified model\n",
    "    X,      # Features\n",
    "    Y,      # Target\n",
    "    cv = 5, # Number of folds\n",
    "    scoring = 'neg_mean_squared_error'  # Metric\n",
    ")\n",
    "\n",
    "print(cv_scores)\n",
    "\n",
    "print(np.mean(-cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02271776 -0.03038288 -0.02430095 -0.03140294 -0.0325379 ]\n",
      "0.028268485556214168\n"
     ]
    }
   ],
   "source": [
    "### CC UPDATE:  In this example, there IS shuffling before the folds are determined, \n",
    "### so 5FCV returns (slightly) different results depending on seed\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# In k-fold CV, the seed matters. But not so much if our dataset is big enough\n",
    "np.random.seed(5)\n",
    "\n",
    "X = default[['balance','income']]\n",
    "Y = default['default']\n",
    "ols = LinearRegression()\n",
    "\n",
    "cv = KFold(\n",
    "    5,              # Number of folds  \n",
    "    shuffle=True,   # Randomizes observations into folds if true\n",
    "    random_state=9  # Affects how randomization occurs\n",
    ")  \n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    ols,     # Specified model\n",
    "    X,       # Features\n",
    "    Y,       # Target\n",
    "    cv = cv, # Number of folds\n",
    "    scoring = 'neg_mean_squared_error'  # Metric\n",
    ")\n",
    "\n",
    "print(cv_scores)\n",
    "\n",
    "print(np.mean(-cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Subset Selection\n",
    "\n",
    "We now switch gears to feature selection. First, let's find the absolute best subset of feature for our dataset.\n",
    "\n",
    "The best subset is defined as the subset of features that predict the target the best. In other words, out of all possible combinations of features, which one has the lowest test error.\n",
    "\n",
    "Likely, the subset of features that will perform best does not contain all available features (model complexity is too high). This is because using all features often leads to overfitting as irrelevant variables improve the training error but harm the test error. Another advantage of feature selection is that by reducing the number of features used, the interpretability of the model increases.\n",
    "\n",
    "Anyways, first we need to get a list of all possible combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['student'], ['balance'], ['income'], ['student', 'balance'], ['student', 'income'], ['balance', 'income'], ['student', 'balance', 'income']]\n"
     ]
    }
   ],
   "source": [
    "def create_comb_betweenX(X):\n",
    "    '''\n",
    "    Returns a list with all possible combinations\n",
    "    of a given list of features.\n",
    "\n",
    "    Input:\n",
    "        - X (lst of str): the features\n",
    "\n",
    "    Output:\n",
    "        - comb (lst of lst): All possible\n",
    "            combinations\n",
    "    '''\n",
    "    interm = []\n",
    "    for i in range(1, len(X)+1):\n",
    "        interm.extend(list(combinations(X, i)))\n",
    "\n",
    "    comb = []\n",
    "    for i in interm:\n",
    "        comb.append(list(i))\n",
    "\n",
    "    return comb\n",
    "\n",
    "comb = create_comb_betweenX(['student', 'balance', 'income'])\n",
    "print(comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a list of all possible combinations that contain between 1 and `p` features. (`p = len(X.columns)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"['student']\": 0.03216106432340415,\n",
       " \"['balance']\": 0.028255145912758118,\n",
       " \"['income']\": 0.03218892697818138,\n",
       " \"['student', 'balance']\": 0.02821718688532552,\n",
       " \"['student', 'income']\": 0.03216771173480891,\n",
       " \"['balance', 'income']\": 0.028225683737308254,\n",
       " \"['student', 'balance', 'income']\": 0.028223354532571528}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['student', 'balance']\n"
     ]
    }
   ],
   "source": [
    "#Let's run one regression for every possible combination and store its value\n",
    "comb_dict = {}\n",
    "\n",
    "for i in comb: # For every combination\n",
    "    X = default[i]\n",
    "    Y = default['default']\n",
    "    ols = LinearRegression()\n",
    "    mse = np.mean(-1*cross_val_score(ols, X, Y, cv = 5,scoring = 'neg_mean_squared_error'))\n",
    "    comb_dict[str(i)] = mse\n",
    "\n",
    "display(comb_dict)\n",
    "print(min(comb_dict, key=comb_dict.get))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the combination of features that best predict whether an individual will default is `student` and `balance`. This hints that `income` is an irrelevant variable when the others are given. This suggests that `income` is highly correlated with at least one of the other variables and thus only adds noise when predicting on the test set.\n",
    "\n",
    "Anyways, now we know which features form the best subset. However, computing all possible combinations is not always feasible. As seen in class, the more features, the larger the data, and the more computationally intensive the model, the longer it takes to compute all possible combinations. Therefore, there is a need for other feature selections methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Absolute Shrinkage and Selection Operator\n",
    "\n",
    "LASSO essentially works just like OLS, but it prefers models with smaller coefficients. In other words, it shrinks the coefficients (towards 0). We use it against our arch nemesis; overfitting! Logically, by shrinking the parameters, LASSO reduces the model flexibility and thus makes overfitting less likely.\n",
    "\n",
    "The LASSO estimation finds $ \\hat{\\beta}^{L} $'s that satisfy:\n",
    "\n",
    "$$ \\min_{\\hat{\\beta}^{L}} \\underbrace{\\sum_{i=1}^{n}\\left(y_{i}-\\sum_{j=0}^{p}\\hat{\\beta}_{j}^{L}x_{ij}\\right)^{2}}_{Loss\\ Function\\ (e.g.,\\  OLS)} + \\underbrace{\\lambda \\sum_{j=1}^{p}\\left|\\hat{\\beta}_{j}\\right|}_{Penalty} $$\n",
    "\n",
    "where $x_{i0} = 1 \\ \\forall\\ i $\n",
    "\n",
    "$ \\lambda $ is the tuning parameter (also named hyperparameter) that determines how much the coefficients get shrunken by. The larger it is, the smaller the coefficients will be as $ \\lambda $ appears positively in the minimization. To determine its optimal value, we use cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 111.6297693672207, tolerance: 0.02543538749999991\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 112.57053471263833, tolerance: 0.025622187500000008\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.41682571830697, tolerance: 0.026275199999999964\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 112.84442466166266, tolerance: 0.025715549999999986\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 112.45965562544134, tolerance: 0.025715549999999986\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 0.028223354532571528, 0.05: 0.02822569959887049, 0.1: 0.028225737636547437, 0.15000000000000002: 0.028225797839311412, 0.2: 0.028225880207162417, 0.25: 0.028225984740100452, 0.30000000000000004: 0.028226111438125524, 0.35000000000000003: 0.028226260301237614, 0.4: 0.02822643132943674, 0.45: 0.0282266245227229, 0.5: 0.02822683988109608, 0.55: 0.0282270774045563, 0.6000000000000001: 0.02822733709310355, 0.65: 0.028227618946737826, 0.7000000000000001: 0.028227922965459135, 0.75: 0.02822824914926747, 0.8: 0.02822859749816283, 0.8500000000000001: 0.02822896801214523, 0.9: 0.028229360691214657, 0.9500000000000001: 0.02822977553537111, 1.0: 0.028230212544614598}\n",
      "(1.0, 0.028230212544614598)\n"
     ]
    }
   ],
   "source": [
    "alpha_dict = {}\n",
    "\n",
    "X = default[['student', 'balance', 'income']]\n",
    "Y = default['default']\n",
    "\n",
    "for al in np.linspace(0, 1, 21):\n",
    "    ols_lasso = Lasso(\n",
    "        alpha=al    # Tuning parameter\n",
    "    )\n",
    "\n",
    "    mse = np.mean(\n",
    "        -1*cross_val_score(\n",
    "            ols_lasso,\n",
    "            X,\n",
    "            Y,\n",
    "            cv = 5,\n",
    "            scoring = 'neg_mean_squared_error'\n",
    "        )\n",
    "    )\n",
    "    alpha_dict[al] = mse\n",
    "\n",
    "print(alpha_dict)\n",
    "print(max(alpha_dict.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the best value for the tuning parameter is $ \\lambda $ = 1. Since this is our upper bound, it indicates that we have not yet converged and you would likely want to try with higher values for $ \\lambda $ as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Stepwise Selection\n",
    "\n",
    "The idea of this model is that we want to add the variable that provides the biggest improvement to our model (the one that yields the smallest error). We start with a model that has a constant as its only feature and continue until we run out of variables or see an increase in our error rate.\n",
    "\n",
    "Its algorithm is fairly intuitive:\n",
    "\n",
    "1. For k = 0, let $ M_{0} $ denote the null model (no Xs)\n",
    "2. For k = 1, ..., p:\n",
    "    - Consider all p - k + 1 models that add one predictor to $ M_{k-1} $\n",
    "    - Pick the best (smallest RSS or largest $ R^{2} $) of these models and call it $ M_{k} $\n",
    "3. Select the single best (CV test error, $ C_{p} $, AIC, BIC, or adjusted $ R^{2} $) model among $ M_{0}$, ..., $ M_{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initial run with only one var (constant term/only bias weight): ['constant']\n",
      "      Benchmark error: 0.032192381250000006\n",
      "\n",
      "\u001b[1mIteration: 0 \u001b[0m\n",
      " Running model with: ['constant', 'student']\n",
      "      Error: 0.03216106432340414\n",
      " Running model with: ['constant', 'balance']\n",
      "      Error: 0.028255145912758118\n",
      " Running model with: ['constant', 'income']\n",
      "      Error: 0.03218892697818138\n",
      "          *** Variable selected: balance\n",
      "          *** Min error selected: 0.028255145912758118\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 1 \u001b[0m\n",
      " Running model with: ['constant', 'balance', 'student']\n",
      "      Error: 0.02821718688532552\n",
      " Running model with: ['constant', 'balance', 'income']\n",
      "      Error: 0.02822568373730826\n",
      "          *** Variable selected: student\n",
      "          *** Min error selected: 0.02821718688532552\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 2 \u001b[0m\n",
      " Running model with: ['constant', 'balance', 'student', 'income']\n",
      "      Error: 0.028223354532571528\n",
      "          \u001b[4m*** No variable was selected \u001b[0m\n",
      "          *** Previous error rate ( 0.02821718688532552 ) is lower than smallest error rate of this iteration ( 0.028223354532571528 )\n",
      "          *** Break\n",
      "\n",
      "Variables chosen for our model ['constant', 'balance', 'student']\n"
     ]
    }
   ],
   "source": [
    "# Add constant to dataframe\n",
    "default['constant'] = 1 \n",
    "\n",
    "# Specify target\n",
    "Y = default['default']\n",
    "\n",
    "# Variables to use in forward propagation\n",
    "vars_left_add = ['student', 'balance', 'income'] \n",
    "\n",
    "# Regression type\n",
    "ols = LinearRegression()\n",
    "\n",
    "# Starting variables (constant initially)\n",
    "current_vars = ['constant']\n",
    "\n",
    "X = default[current_vars]\n",
    "benchmark_error = np.mean(-1*cross_val_score(ols, X, Y, cv = 5, scoring = 'neg_mean_squared_error'))\n",
    "print(' Initial run with only one var (constant term/only bias weight):', current_vars)\n",
    "print('      Benchmark error:', benchmark_error)\n",
    "print('')\n",
    "\n",
    "# Keep adding the best variables (until no improvement can be made)\n",
    "for iter in range(len(vars_left_add)):\n",
    "    print('\\033[1m'+ 'Iteration:', iter, '\\033[0m')\n",
    "    error_list = []\n",
    "    # Iterate over all the variables left to add\n",
    "    for var in vars_left_add:\n",
    "        # Modify X according to current iteration\n",
    "        X = default[current_vars + [var]]\n",
    "        # Perform 5-fold CV to get errors\n",
    "        error = np.mean(-1*cross_val_score(ols, X, Y, cv = 5, scoring = 'neg_mean_squared_error'))\n",
    "        error_list.append(error)\n",
    "        print(' Running model with:', current_vars + [var])\n",
    "        print('      Error:', error)\n",
    "\n",
    "    # Chose the smallest error\n",
    "    min_error = min(error_list)\n",
    "    chosen_col_index = error_list.index(min_error)\n",
    "\n",
    "    # If our current smallest error is smaller than our previous error, than we add a variable\n",
    "    if min_error < benchmark_error:\n",
    "        print('          *** Variable selected:', vars_left_add[chosen_col_index])\n",
    "        print('          *** Min error selected:', min_error)\n",
    "        print('          *** Chose the variable that generated the min error + was lower than previous error')\n",
    "        print('')\n",
    "        # Add the variable that produced the smallest error to current_vars\n",
    "        current_vars.append(vars_left_add[chosen_col_index])\n",
    "        # Delete chosen variable from vars_left_add\n",
    "        del vars_left_add[chosen_col_index] \n",
    "        # Update benchmark_error\n",
    "        benchmark_error = min_error\n",
    "    \n",
    "    # Otherwise, we stop our model\n",
    "    else:\n",
    "        print('          \\033[4m*** No variable was selected', '\\033[0m')\n",
    "        print('          *** Previous error rate (', benchmark_error,') is lower than smallest error rate of this iteration (', min_error ,')')\n",
    "        print('          *** Break')\n",
    "        break\n",
    "\n",
    "print('')\n",
    "print('Variables chosen for our model', current_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that forward stepwise selection chooses the model with a constant, `balance`, and `student` as the features while excluding `income`. luckily, this coincides with the best subset selection we performed previously. However, there is no guarantee that we do find the best model from all possible combinations with forward stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Stepwise Selection\n",
    "The idea of this model is that we want to remove the variable that is not adding predictive power to our model (one that we can remove without increasing our error). We start with a full model (using all of our variables) and drop variables until we see an increase in our error rate or end up with only a constant. The idea remains the exact same as in forward stepwise selection, but we start at a different point.\n",
    "\n",
    "The algorithm goes as follows:\n",
    "\n",
    "1. For k = p, let $ M_{p} $ denote the fully specified model (all Xs)\n",
    "2. For k = p-1, ..., 0:\n",
    "    - Consider all p - k + 1 models that remove one predictor from $ M_{k+1} $\n",
    "    - Pick the best (smallest RSS or largest $ R^{2} $) of these models and call it $ M_{k} $\n",
    "3. Select the single best (CV test error, $ C_{p} $, AIC, BIC, or adjusted $ R^{2} $) model among $ M_{0}$, ..., $ M_{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initial run with all vars: ['constant', 'student', 'balance', 'income']\n",
      "      Benchmark error: 0.028223354532571465\n",
      "\n",
      "\u001b[1mIteration: 0 \u001b[0m\n",
      " Running model with: ['constant', 'balance', 'income']\n",
      "      Error: 0.02822568373730826\n",
      " Running model with: ['constant', 'student', 'income']\n",
      "      Error: 0.032167711734808854\n",
      " Running model with: ['constant', 'student', 'balance']\n",
      "      Error: 0.028217186885325513\n",
      "          *** Will drop: income\n",
      "          *** Min error selected: 0.028217186885325513\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 1 \u001b[0m\n",
      " Running model with: ['constant', 'balance']\n",
      "      Error: 0.028255145912758118\n",
      " Running model with: ['constant', 'student']\n",
      "      Error: 0.03216106432340414\n",
      "          \u001b[4m*** No variable was selected \u001b[0m\n",
      "          *** Previous error rate ( 0.028217186885325513 ) is lower than smallest error rate of this iteration ( 0.028255145912758118 )\n",
      "          *** Break\n",
      "\n",
      "Variables chosen for our model ['constant', 'student', 'balance']\n"
     ]
    }
   ],
   "source": [
    "# Add constant to dataframe\n",
    "default['constant'] = 1 \n",
    "\n",
    "# Specify the target\n",
    "Y = default['default']\n",
    "\n",
    "# Variables to remove iteratively\n",
    "vars_left_to_drop = ['student', 'balance', 'income'] \n",
    "\n",
    "# Regression type\n",
    "ols = LinearRegression()\n",
    "\n",
    "# Starting variables (all)\n",
    "current_vars = ['constant'] + vars_left_to_drop\n",
    "\n",
    "X = default[current_vars]\n",
    "benchmark_error = np.mean(-1*cross_val_score(ols, X, Y, cv = 5, scoring = 'neg_mean_squared_error'))\n",
    "print(' Initial run with all vars:', current_vars)\n",
    "print('      Benchmark error:', benchmark_error)\n",
    "print('')\n",
    "\n",
    "# Keep removing the worst variables (until no improvement can be made)\n",
    "for iter in range(len(vars_left_to_drop)):\n",
    "    print('\\033[1m'+ 'Iteration:', iter, '\\033[0m')\n",
    "    error_list = []\n",
    "    # Iterate over all the variables left to remove\n",
    "    for var in vars_left_to_drop:\n",
    "        # Modify X according to current iteration\n",
    "        vars_to_be_used = ['constant'] + [i for i in vars_left_to_drop if i != var]\n",
    "        X = default[['constant'] + [i for i in vars_left_to_drop if i != var]]\n",
    "        # Perform 5-fold CV to get errors\n",
    "        error = np.mean(-1*cross_val_score(ols, X, Y, cv = 5, scoring = 'neg_mean_squared_error'))\n",
    "        error_list.append(error)\n",
    "        print(' Running model with:', vars_to_be_used)\n",
    "        print('      Error:', error)\n",
    "\n",
    "    # Chose the largest error\n",
    "    min_error = min(error_list)\n",
    "    chosen_col_index = error_list.index(min_error)\n",
    "\n",
    "    # If our current smallest error is smaller than our previous error, then we drop the variable associated with it\n",
    "    if min_error < benchmark_error:\n",
    "        print('          *** Will drop:', vars_left_to_drop[chosen_col_index])\n",
    "        print('          *** Min error selected:', min_error)\n",
    "        print('          *** Chose the variable that generated the min error + was lower than previous error')\n",
    "        print('')\n",
    "        # Delete chosen variable from current_vars and vars_left_to_drop\n",
    "        del current_vars[chosen_col_index + 1]\n",
    "        del vars_left_to_drop[chosen_col_index]\n",
    "        # Update benchmark_error\n",
    "        benchmark_error = min_error\n",
    "    \n",
    "    # If not, we keep our model\n",
    "    else:\n",
    "        print('          \\033[4m*** No variable was selected', '\\033[0m')\n",
    "        print('          *** Previous error rate (', benchmark_error,') is lower than smallest error rate of this iteration (', min_error ,')')\n",
    "        print('          *** Break')\n",
    "        break\n",
    "\n",
    "print('')\n",
    "print('Variables chosen for our model', current_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as best subset and forward stepwise selection, backward stepwise selection chooses the model with a constant, `balance`, and `student` as the features while excluding `income`. However, there is no guarantee that their choices are the same. Indeed, as `p` becomes larger, it becomes increasingly likely for forward and backward stepwise selection to yield different models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
